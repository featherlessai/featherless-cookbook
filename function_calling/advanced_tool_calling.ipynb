{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Simulated Tool Calling with Open Models\n",
    "\n",
    "This notebook demonstrates advanced patterns for simulating function (tool) calling with open models using the OpenAI Python SDK. \n",
    "\n",
    "Unlike the basic examples, these scenarios require the model to output an *array* of JSON objects (for multiple function calls in a single response, chained reasoning, or batch calls). \n",
    "\n",
    "Because the OpenAI SDK's `response_format={\"type\": \"json_object\"}` only supports a single JSON object (not an array), we **do not** use `response_format` here. Instead, we rely on careful prompt engineering to instruct the model to output a valid JSON array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "1. Sign up for an account at [Featherless](https://featherless.ai/register)\n",
    "2. Subscribe to a plan and get your API key from [API Keys](https://featherless.ai/account/api-keys)\n",
    "\n",
    "## Setup\n",
    "First, import the required libraries and set up your API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the OpenAI Python SDK if you haven't already\n",
    "# !pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Initialize the OpenAI client with your endpoint and API key\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api.featherless.ai/v1\",\n",
    "    api_key=\"YOUR FEATHERLESS API KEY\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Example 4: Multiple Function Calls in One Response\n",
    "\n",
    "In this example, the model is instructed to output an **array** of function calls (e.g., both `get_weather` and `get_time` for multiple cities). \n",
    "\n",
    "**Note:** We do *not* use `response_format={\"type\": \"json_object\"}` because the response is a JSON array, not a single object. Instead, we rely on prompt instructions to ensure the output is a valid JSON array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulated Tool Calling (Multiple Calls) Output: [\n",
      "  {\"function\": \"get_weather\", \"arguments\": {\"location\": \"London, UK\"}},\n",
      "  {\"function\": \"get_time\", \"arguments\": {\"location\": \"London, UK\"}},\n",
      "  {\"function\": \"get_weather\", \"arguments\": {\"location\": \"Paris, France\"}},\n",
      "  {\"function\": \"get_time\", \"arguments\": {\"location\": \"Paris, France\"}}\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You are a function calling assistant. \"\n",
    "                \"You have access to two functions:\\n\"\n",
    "                '1. get_weather(location: str)\\n'\n",
    "                '2. get_time(location: str)\\n'\n",
    "                \"When asked a question, respond ONLY in the following JSON format:\\n\"\n",
    "                '[\\n'\n",
    "                '  {\"function\": \"get_weather\", \"arguments\": {\"location\": \"<city, country>\"}},\\n'\n",
    "                '  {\"function\": \"get_time\", \"arguments\": {\"location\": \"<city, country>\"}},\\n'\n",
    "                '  ...\\n'\n",
    "                ']\\n'\n",
    "                \"If only one function is needed, return a list with one item. \"\n",
    "                \"Do not answer the question directly. Only output the JSON array.\"\n",
    "            ),\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What's the weather and time in London and Paris?\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "print(\"Simulated Tool Calling (Multiple Calls) Output:\", response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Example 5: Chained Reasoning (Dependent Function Calls)\n",
    "\n",
    "This example demonstrates *chained reasoning*: the model must first call `get_location_of_event` to get a location, then use that result as input to `get_weather`. \n",
    "\n",
    "Again, the output is a JSON array, so we do not use `response_format`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulated Tool Calling (Chained Reasoning) Output: [\n",
      "  {\"function\": \"get_location_of_event\", \"arguments\": {\"event\": \"Wimbledon tennis final\"}},\n",
      "  {\"function\": \"get_weather\", \"arguments\": {\"location\": \"<location_from_previous_call>\"}},\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You are a function calling assistant. \"\n",
    "                \"You have access to two functions:\\n\"\n",
    "                '1. get_location_of_event(event: str) -> location: str\\n'\n",
    "                '2. get_weather(location: str)\\n'\n",
    "                \"If the user asks about the weather for an event, first call get_location_of_event, \"\n",
    "                \"then use its output as the location argument for get_weather. \"\n",
    "                \"Respond ONLY in the following JSON format:\\n\"\n",
    "                '[\\n'\n",
    "                '  {\"function\": \"get_location_of_event\", \"arguments\": {\"event\": \"<event_name>\"}},\\n'\n",
    "                '  {\"function\": \"get_weather\", \"arguments\": {\"location\": \"<location_from_previous_call>\"}},\\n'\n",
    "                ']\\n'\n",
    "                \"Do not answer the question directly. Only output the JSON array.\"\n",
    "            ),\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What's the weather like at the Wimbledon tennis final?\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "print(\"Simulated Tool Calling (Chained Reasoning) Output:\", response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Example 6: Function with Multiple Arguments\n",
    "\n",
    "This example shows how to simulate a function that takes multiple arguments. The output is a single JSON object, so you *could* use `response_format`, but here we show the prompt-based approach for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulated Tool Calling (Multiple Arguments) Output: {\n",
      "  \"function\": \"get_flight_status\",\n",
      "  \"arguments\": {\"flight_number\": \"BA123\", \"date\": \"2024-06-10\"}\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You are a function calling assistant. \"\n",
    "                \"You have access to a function:\\n\"\n",
    "                'get_flight_status(flight_number: str, date: str)\\n'\n",
    "                \"When asked, respond ONLY in the following JSON format:\\n\"\n",
    "                '{\\n  \"function\": \"get_flight_status\",\\n  \"arguments\": {\"flight_number\": \"<flight_number>\", \"date\": \"<YYYY-MM-DD>\"}\\n}\\n'\n",
    "                \"Do not answer the question directly. Only output the JSON.\"\n",
    "            ),\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What's the status of flight BA123 on June 10th?\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "print(\"Simulated Tool Calling (Multiple Arguments) Output:\", response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Example 7: Batch Calls for Multiple Cities\n",
    "\n",
    "This example demonstrates how to ask the model to output a JSON array of function calls for multiple cities in a single response. Again, we do not use `response_format`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulated Tool Calling (Batch Calls) Output: [\n",
      "  {\"function\": \"get_weather\", \"arguments\": {\"location\": \"Berlin, Germany\"}},\n",
      "  {\"function\": \"get_weather\", \"arguments\": {\"location\": \"Madrid, Spain\"}},\n",
      "  {\"function\": \"get_weather\", \"arguments\": {\"location\": \"Rome, Italy\"}}\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You are a function calling assistant. \"\n",
    "                \"You have access to a function:\\n\"\n",
    "                'get_weather(location: str)\\n'\n",
    "                \"When asked about multiple cities, respond ONLY in the following JSON format:\\n\"\n",
    "                '[\\n'\n",
    "                '  {\"function\": \"get_weather\", \"arguments\": {\"location\": \"<city1, country1>\"}},\\n'\n",
    "                '  {\"function\": \"get_weather\", \"arguments\": {\"location\": \"<city2, country2>\"}},\\n'\n",
    "                '  ...\\n'\n",
    "                ']\\n'\n",
    "                \"Do not answer the question directly. Only output the JSON array.\"\n",
    "            ),\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What's the weather in Berlin, Madrid, and Rome?\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "print(\"Simulated Tool Calling (Batch Calls) Output:\", response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "- For advanced tool calling scenarios (multiple calls, chained reasoning, batch calls), instruct the model to output a JSON array.\n",
    "- Do **not** use `response_format={\"type\": \"json_object\"}` when expecting a JSON array, as it only supports single objects.\n",
    "- Use clear prompt engineering to ensure the model outputs valid JSON arrays for downstream parsing.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Example 8: Named JSON List with `response_format={\"type\": \"json_object\"}`\n",
    "\n",
    "If you want to return multiple function calls but still use the `response_format={\"type\": \"json_object\"}` parameter, you can instruct the model to return a *named* list (e.g., `{ \"calls\": [ ... ] }`).\n",
    "\n",
    "This works because the top-level structure is a JSON object, not an array. This is a good compromise if you want to leverage OpenAIâ€™s response validation for JSON objects, but still need to return multiple calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulated Tool Calling (Named JSON List) Output: {\n",
      "  \"calls\": [\n",
      "    {\"function\": \"get_weather\", \"arguments\": {\"location\": \"London, UK\"}},\n",
      "    {\"function\": \"get_time\", \"arguments\": {\"location\": \"London, UK\"}},\n",
      "    {\"function\": \"get_weather\", \"arguments\": {\"location\": \"Paris, France\"}},\n",
      "    {\"function\": \"get_time\", \"arguments\": {\"location\": \"Paris, France\"}}\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You are a function calling assistant. \"\n",
    "                \"You have access to two functions:\\n\"\n",
    "                '1. get_weather(location: str)\\n'\n",
    "                '2. get_time(location: str)\\n'\n",
    "                \"When asked a question, respond ONLY in the following JSON format:\\n\"\n",
    "                '{\\n  \"calls\": [\\n    {\"function\": \"get_weather\", \"arguments\": {\"location\": \"<city, country>\"}},\\n    {\"function\": \"get_time\", \"arguments\": {\"location\": \"<city, country>\"}}\\n  ]\\n}\\n'\n",
    "                \"If only one function is needed, return a list with one item. \"\n",
    "                \"Do not answer the question directly. Only output the JSON object.\"\n",
    "            ),\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What's the weather and time in London and Paris?\"\n",
    "        }\n",
    "    ],\n",
    "    response_format={\"type\": \"json_object\"},\n",
    ")\n",
    "print(\"Simulated Tool Calling (Named JSON List) Output:\", response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Example 9: Chained Reasoning with Named JSON List\n",
    "\n",
    "You can also use a named list for chained reasoning scenarios. This allows you to keep the output as a valid JSON object for the SDK, while still representing multiple dependent calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulated Tool Calling (Chained Reasoning, Named List) Output: {\n",
      "  \"calls\": [\n",
      "    {\"function\": \"get_location_of_event\", \"arguments\": {\"event\": \"Wimbledon tennis final\"}},\n",
      "    {\"function\": \"get_weather\", \"arguments\": {\"location\": \"<location_from_previous_call>\"}}\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You are a function calling assistant. \"\n",
    "                \"You have access to two functions:\\n\"\n",
    "                '1. get_location_of_event(event: str) -> location: str\\n'\n",
    "                '2. get_weather(location: str)\\n'\n",
    "                \"If the user asks about the weather for an event, first call get_location_of_event, \"\n",
    "                \"then use its output as the location argument for get_weather. \"\n",
    "                \"Respond ONLY in the following JSON format:\\n\"\n",
    "                '{\\n  \"calls\": [\\n    {\"function\": \"get_location_of_event\", \"arguments\": {\"event\": \"<event_name>\"}},\\n    {\"function\": \"get_weather\", \"arguments\": {\"location\": \"<location_from_previous_call>\"}}\\n  ]\\n}\\n'\n",
    "                \"Do not answer the question directly. Only output the JSON object.\"\n",
    "            ),\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What's the weather like at the Wimbledon tennis final?\"\n",
    "        }\n",
    "    ],\n",
    "    response_format={\"type\": \"json_object\"},\n",
    ")\n",
    "print(\"Simulated Tool Calling (Chained Reasoning, Named List) Output:\", response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Example 10: Batch Calls for Multiple Cities with Named JSON List\n",
    "\n",
    "This example demonstrates how to ask the model to output a named JSON list of function calls for multiple cities, while still using `response_format={\"type\": \"json_object\"}`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulated Tool Calling (Batch Calls, Named List) Output: {\n",
      "  \"calls\": [\n",
      "    {\"function\": \"get_weather\", \"arguments\": {\"location\": \"Berlin, Germany\"}},\n",
      "    {\"function\": \"get_weather\", \"arguments\": {\"location\": \"Madrid, Spain\"}},\n",
      "    {\"function\": \"get_weather\", \"arguments\": {\"location\": \"Rome, Italy\"}}\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You are a function calling assistant. \"\n",
    "                \"You have access to a function:\\n\"\n",
    "                'get_weather(location: str)\\n'\n",
    "                \"When asked about multiple cities, respond ONLY in the following JSON format:\\n\"\n",
    "                '{\\n  \"calls\": [\\n    {\"function\": \"get_weather\", \"arguments\": {\"location\": \"<city1, country1>\"}},\\n    {\"function\": \"get_weather\", \"arguments\": {\"location\": \"<city2, country2>\"}}\\n  ]\\n}\\n'\n",
    "                \"Do not answer the question directly. Only output the JSON object.\"\n",
    "            ),\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What's the weather in Berlin, Madrid, and Rome?\"\n",
    "        }\n",
    "    ],\n",
    "    response_format={\"type\": \"json_object\"},\n",
    ")\n",
    "print(\"Simulated Tool Calling (Batch Calls, Named List) Output:\", response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
