{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Simulated Tool Calling Example (Back-and-Forth)\n",
    "\n",
    "This notebook demonstrates a full, end-to-end example of **simulated tool calling** using an OpenAI-compatible API (such as Featherless), *without* native tool calling support.\n",
    "\n",
    "The LLM suggests a tool call by outputting a JSON object, the code executes the tool, and the result is provided back to the LLM for a final answer.\n",
    "\n",
    "This pattern is inspired by the [OpenRouter tool calling documentation](https://openrouter.ai/docs/features/tool-calling), but uses prompt-based simulated tool calls instead of the OpenAI `tools` parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "1. Sign up for an account at [Featherless](https://featherless.ai/register)\n",
    "2. Subscribe to a plan and get your API key from [API Keys](https://featherless.ai/account/api-keys)\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openai requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api.featherless.ai/v1\",\n",
    "    api_key=\"YOUR FEATHERLESS API KEY\",\n",
    ")\n",
    "\n",
    "def search_gutenberg_books(search_terms):\n",
    "    search_query = \" \".join(search_terms)\n",
    "    url = \"https://gutendex.com/books\"\n",
    "    response = requests.get(url, params={\"search\": search_query})\n",
    "    simplified_results = []\n",
    "    for book in response.json().get(\"results\", []):\n",
    "        simplified_results.append({\n",
    "            \"id\": book.get(\"id\"),\n",
    "            \"title\": book.get(\"title\"),\n",
    "            \"authors\": book.get(\"authors\"),\n",
    "        })\n",
    "    return simplified_results\n",
    "\n",
    "TOOL_MAPPING = {\n",
    "    \"search_gutenberg_books\": search_gutenberg_books\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: User asks a question\n",
    "\n",
    "We'll use the following task:\n",
    "\n",
    "> What are the titles of some James Joyce books?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"What are the titles of some James Joyce books?\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": task},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Simulate a tool call (model outputs a JSON object)\n",
    "\n",
    "We instruct the model to output a JSON object describing the tool call it wants to make. We use `response_format={\"type\": \"json_object\"}` to ensure valid JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model tool call: {'function': 'search_gutenberg_books', 'arguments': {'search_terms': ['James Joyce']}}\n"
     ]
    }
   ],
   "source": [
    "tool_call_prompt = (\n",
    "    \"You have access to a function: search_gutenberg_books(search_terms: list of str).\\n\"\n",
    "    \"When asked about books, respond ONLY in the following JSON format:\\n\"\n",
    "    '{\\n  \"function\": \"search_gutenberg_books\",\\n  \"arguments\": {\"search_terms\": [\"<term1>\", \"<term2>\"]}\\n}\\n'\n",
    "    \"Do not answer the question directly. Only output the JSON.\"\n",
    ")\n",
    "\n",
    "messages_tool = [\n",
    "    {\"role\": \"system\", \"content\": tool_call_prompt},\n",
    "    {\"role\": \"user\", \"content\": task},\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    messages=messages_tool,\n",
    "    response_format={\"type\": \"json_object\"},\n",
    ")\n",
    "tool_call_json = json.loads(response.choices[0].message.content)\n",
    "print(\"Model tool call:\", tool_call_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Execute the tool call and append the result as a message\n",
    "\n",
    "We parse the tool call, execute the function, and append the result as a `tool` message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool result (truncated): [{'id': 4300, 'title': 'Ulysses', 'authors': [{'name': 'Joyce, James', 'birth_year': 1882, 'death_year': 1941}]}, {'id': 2814, 'title': 'Dubliners', 'authors': [{'name': 'Joyce, James', 'birth_year': 1882, 'death_year': 1941}]}]\n"
     ]
    }
   ],
   "source": [
    "tool_name = tool_call_json[\"function\"]\n",
    "tool_args = tool_call_json[\"arguments\"]\n",
    "tool_result = TOOL_MAPPING[tool_name](**tool_args)\n",
    "\n",
    "messages_with_tool = [\n",
    "    {\"role\": \"system\", \"content\": tool_call_prompt},\n",
    "    {\"role\": \"user\", \"content\": task},\n",
    "    {\"role\": \"tool\", \"name\": tool_name, \"content\": json.dumps(tool_result)},\n",
    "]\n",
    "print(\"Tool result (truncated):\", tool_result[:2])  # Show just a couple of results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Ask the model to answer the original question using the tool result\n",
    "\n",
    "We now prompt the model to use the tool result to answer the user's question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final answer:\n",
      " The titles of some James Joyce books are:\n",
      "\n",
      "1. Ulysses\n",
      "2. Dubliners\n",
      "3. A Portrait of the Artist as a Young Man\n",
      "4. Index of the Project Gutenberg Works of James Joyce\n",
      "5. Chamber Music\n",
      "6. Exiles: A Play in Three Acts\n"
     ]
    }
   ],
   "source": [
    "answer_prompt = (\n",
    "    \"You are a helpful assistant.\\n\"\n",
    "    \"You have access to the results of a function call.\\n\"\n",
    "    \"Use the tool result to answer the user's original question.\"\n",
    ")\n",
    "\n",
    "messages_final = [\n",
    "    {\"role\": \"system\", \"content\": answer_prompt},\n",
    "    {\"role\": \"user\", \"content\": task},\n",
    "    {\"role\": \"tool\", \"name\": tool_name, \"content\": json.dumps(tool_result)},\n",
    "]\n",
    "\n",
    "response_final = client.chat.completions.create(\n",
    "    model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    messages=messages_final,\n",
    ")\n",
    "print(\"\\nFinal answer:\\n\", response_final.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "- The LLM suggests a tool call by outputting a JSON object (simulated tool calling)\n",
    "- The code executes the tool and appends the result as a `tool` message\n",
    "- The LLM uses the tool result to answer the user's original question\n",
    "\n",
    "This pattern works with any OpenAI-compatible endpoint, even if it does not support native tool calling."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
