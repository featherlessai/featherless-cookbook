{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulated Function Calling with Open Models and OpenAI Python SDK\n",
    "\n",
    "This notebook demonstrates how to simulate function (tool) calling with Llama models using the OpenAI Python SDK.  \n",
    "This is an alternative to OpenAI's built-in function calling, and works with any model that can follow instructions and output JSON.\n",
    "\n",
    "We use the `response_format={\"type\": \"json_object\"}` parameter to ensure the model outputs valid JSON, and we instruct the model via the system prompt to only output the required JSON structure.\n",
    "\n",
    "> **Note:** This approach is especially useful for open models that do not natively support OpenAI's function calling API.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "1. Sign up for an account at [Featherless](https://featherless.ai/register)\n",
    "2. Subscribe to a plan and get your API key from [API Keys](https://featherless.ai/account/api-keys)\n",
    "## Setup\n",
    "First, let's import the required libraries and set up our API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the OpenAI Python SDK if you haven't already\n",
    "# !pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Initialize the OpenAI client with your endpoint and API key\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api.featherless.ai/v1\",\n",
    "    api_key=\"YOUR FEATHERLESS API KEY\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Simulate a Single Function Call (`get_weather`)\n",
    "\n",
    "We instruct the model to always respond with a JSON object calling the `get_weather` function, with a `location` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='4XjrRd', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='{\\n  \"function\": \"get_weather\",\\n  \"arguments\": {\"location\": \"Paris, France\"}\\n}', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None))], created=1748615619430, model='meta-llama/Meta-Llama-3.1-8B-Instruct', object='chat.completion', service_tier=None, system_fingerprint='', usage=CompletionUsage(completion_tokens=22, prompt_tokens=71, total_tokens=93, completion_tokens_details=None, prompt_tokens_details=None))\n",
      "Simulated Tool Calling Output: {\n",
      "  \"function\": \"get_weather\",\n",
      "  \"arguments\": {\"location\": \"Paris, France\"}\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You are a function calling assistant. \"\n",
    "                \"When asked a question, respond ONLY in the following JSON format:\\n\"\n",
    "                '{\\n  \"function\": \"get_weather\",\\n  \"arguments\": {\"location\": \"<city, country>\"}\\n}\\n'\n",
    "                \"Do not answer the question directly. Only output the JSON.\"\n",
    "            ),\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is the weather like in Paris today?\"\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "print(response)\n",
    "print(\"Simulated Tool Calling Output:\", response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Simulate Multiple Functions (`get_weather` or `get_time`)\n",
    "\n",
    "Here, we tell the model it can call either `get_weather` or `get_time`, and to always respond with the correct JSON for the function it chooses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulated Tool Calling (Multiple Functions) Output: {\n",
      "  \"function\": \"get_weather\",\n",
      "  \"arguments\": {\"location\": \"Tokyo, Japan\"}\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You are a function calling assistant. \"\n",
    "                \"You have access to two functions:\\n\"\n",
    "                '1. get_weather(location: str)\\n'\n",
    "                '2. get_time(location: str)\\n'\n",
    "                \"When asked a question, respond ONLY in the following JSON format:\\n\"\n",
    "                '{\\n  \"function\": \"<function_name>\",\\n  \"arguments\": {\"location\": \"<city, country>\"}\\n}\\n'\n",
    "                \"Do not answer the question directly. Only output the JSON.\"\n",
    "            ),\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What's the weather and time in Tokyo?\"\n",
    "        }\n",
    "    ],\n",
    "    response_format={\"type\": \"json_object\"},\n",
    ")\n",
    "print(\"Simulated Tool Calling (Multiple Functions) Output:\", response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Force a Specific Function (`get_time`)\n",
    "\n",
    "You can force the model to always call a specific function, regardless of the user's question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulated Tool Calling (Forced Function) Output: {\n",
      "  \"function\": \"get_time\",\n",
      "  \"arguments\": {\"location\": \"New York, USA\"}\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You are a function calling assistant. \"\n",
    "                \"You must always call the function get_time. \"\n",
    "                \"Respond ONLY in the following JSON format:\\n\"\n",
    "                '{\\n  \"function\": \"get_time\",\\n  \"arguments\": {\"location\": \"<city, country>\"}\\n}\\n'\n",
    "                \"Do not answer the question directly. Only output the JSON.\"\n",
    "            ),\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Tell me the time in New York.\"\n",
    "        }\n",
    "    ],\n",
    "    response_format={\"type\": \"json_object\"},\n",
    ")\n",
    "print(\"Simulated Tool Calling (Forced Function) Output:\", response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- This approach works with any model that can follow instructions and output JSON, including open models.\n",
    "- You can simulate function calling by carefully crafting your system prompt and using `response_format={\"type\": \"json_object\"}`.\n",
    "- This is a flexible alternative to OpenAI's built-in function calling, especially for open models or custom endpoints.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
